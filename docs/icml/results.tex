%!TEX root = paper.tex

\section{Results}
\label{sec:org99c3fc8}
In the following, we evaluate the forecast quality of our method compared to
multiple state-of-the-art forecasts for confirmed cases on
county-level. All comparison forecasts are collected from the COVID-19 Forecast
Hub\footnote{\url{https://github.com/reichlab/covid19-forecast-hub}} as submitted by
the respective teams. The COVID-19 Forecast Hub features county-level forecasts
from July 5th onwards and we selected those models for which at least 10 forecasts
where available since then. The full list of comparison forecasts is shown in
\Cref{tab:forecasts}.

\begin{table*}[t]
\small
\caption{Forecasting models for confirmed cases on county-level.\label{tab:forecasts}}
\centering
\begin{tabular}{lll}
\toprule
\bf Group & \bf Model \\
\midrule
Center for Disease Dynamics, Economics \& Policy & \it CDDP-SEIR\_MCMC & \citep{cddep_seir_mcmc} \\
Columbia University & \it CU-* & \citep{forecasts/columbia} \\
COVID Alliance at MIT & \it MITCovAlliance-SIR & \citep{baek2020limits} \\
Iowa State University Lily Wang Research Group & \it IowaStateLW-STEM & \citep{wang2020spatiotemporal} \\
Johns Hopkins ID Dynamics COVID-19 Working Group & \it JHU-IDD\_CovidSP & \citep{forecasts/jhu_idd_covidsp} \\
LockNQuay & \it LNQ-ens1 & \citep{forecasts/lnq_ens1} \\
Oliver Wyman & \it Pandemic Navigator & \citep{forecasts/oliver_wyman} \\
UCLA Statistical Machine Learning Lab & \it UCLA-SuEIR & \citep{forecasts/Zou2020.05.24.20111989} \\
University of Southern California Data Science Lab & \it USC-SI\_kJalpha & \citep{srivastava2020fast} \\
University of Massachusetts Amherst & \it UMass-MechBayes & \citep{forecasts/umass_mechbayes} \\
Google and Harvard University & \it Google\_Harvard-CPF & \citep{forecasts/google} \\
Microsoft & \it Microsoft-DeepSTIA & \citep{forecasts/microsoft} \\
University of Virginia & \it UVA-Ensemble & \citep{forecasts/uva} \\
University of Georgia & \it CEID-Walk & \citep{forecasts/ceid} \\
Los Alamos National Labs & \it LANL-GrowthRate & \citep{forecasts/lanl} \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Forecast setup and model selection} To compute forecasts for the
different dates in the test set, we use the following fully automated model
selection scheme: For each forecast date \(d\), we perform cross-validation by
holding out additional 21 days of validation data and train the model on the
remaining data. We then select the best hyperparameters as measured by RMSE on
the validation set and retrain the whole model with those hyperparameters on the
combined training and validation set to compute the final forecast. When
computing the forecasts, we hold all additional input data (e.g., symptom
survey, mobility, weather, etc.) constant after the last observed day
\(d\).\footnote{This setting places natural limits on the duration of the forecasting
horizon. We reserve the joint forecasting of cases and covariates -- what could
extend the horizon -- for future work.}. For all training details of the model,
please see the supplementary material.


\paragraph{Input data} As input features for \bAR, we use multiple data sources
as listed in \Cref{tab:data-sources}. Confirmed cases enter the model only in the
autoregressive part. All other covariates enter the model only as input features
for the time-varying \(\beta\)-part. For cases and weather data, we use the
preprocessed data from the Google COVID-19 Open Data repository
\citep{data/Wahltinez2020}. All datasets are publicly available, de-identified,
and aggregated at county- or state-level.

\begin{table*}[t]
\small
\caption{Data sources for \bAR.\label{tab:data-sources}}
\centering
\begin{tabular}{lll}
\toprule
\bf Dataset & \bf Source & \bf Resolution \\
\midrule
Confirmed Cases &  \citet{data/nytimes_cases} &  County \\
& \multicolumn{1}{l}{\it Confirmed cases based on reports from state \& local health agencies} \\
\midrule
Symptom Survey & CMU COVIDcast \citep{data/covidcast} & County, State \\
& \citet{data/fb_symptom_survey} \\
& \multicolumn{1}{l}{\it Prevalence of COVID-like symptoms from self-reported surveys} \\
\midrule
Movement Range Maps &  \citet{data/fb_movement_range} &  County, State \\
& \multicolumn{1}{l}{\it Mobility metrics related to physical distancing measures} \\
& \multicolumn{1}{l}{\it (change in movement and staying put)} \\
\midrule
Community Mobility & \citet{data/google_mobility} &  County, State \\
& \multicolumn{1}{l}{\it Movement trends across different categories of places} \\
& \multicolumn{1}{l}{\it (retail and recreation, groceries and pharmacies, etc.)} \\
\midrule
Doctor visits & CMU COVIDcast \citep{data/covidcast} & County, State \\
& \multicolumn{1}{l}{Percentage of COVID-related doctorâ€™s visits in a given location} \\
\midrule
Testing &  \citet{data/covidtracking} & State \\
& \multicolumn{1}{l}{\it Total number of COVID PCR tests per state} \\
\midrule
Weather & NOAA GHCN \citep{data/menne2012overview} &  County \\
& \multicolumn{1}{l}{\it Average, minimum, maximum temperature \& rainfall per county} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{img/us_rank_mae.png}
  \vspace{-2ex}
\caption{Ranking of county-level forecasts by average MAE over various forecast horizons. The proposed neural relational autoregressive model (\bAR) shows strong performance over all horizons when compared to state-of-the-art forecasts. Mean rank over all horizons in parentheses.}
\label{fig:ranking-covidhub-mae}
\end{figure}



\paragraph{Forecast evaluation} \Cref{fig:mae-covidhub} shows the forecast
quality as measured by MAE for multiple forecast horizons.\footnote{MAE numbers are computed in accordance with \url{https://github.com/youyanggu/covid19-forecast-hub-evaluation}} It can be seen that
the proposed \bAR models shows a consistently strong performance and is for all
forecasting dates and horizons either the best model or among the best.
\Cref{fig:ranking-covidhub-mae}, which shows the ranking of all models by the
average MAE for each forecast horizon, further illustrates this property. It can
be seen that \bAR model is consistently ranked first over all horizons.
Furthermore, other models show much larger variability in their performance.

\begin{figure*}[t]
\centering
\includegraphics[width=.75\linewidth]{img/us_mae/us_mae.png}
\caption{\label{fig:mae-covidhub}Comparison of \bAR model (blue) to 15 county-level models from COVID-19 forecast hub (gray). Forecast quality is measured in MAE (log-scale) where the absolute errors are averaged over all counties. For similar analysis using RMSE please see the supplementary material.}
\end{figure*}


To also evaluate the performance of our model on days prior to July 5th, we
compare to forecasts of Google Cloud AI \citep{arik2020interpretable} and Columbia
University \citep{forecasts/columbia} which provide county-level forecasts of
confirmed cases from May 11th to June 27th. \Cref{fig:mae-google} shows the average
MAE over all counties for 7 and 14 day forecasts for these models.\footnote{For this
comparison, average MAE is computed as described in \citep{arik2020interpretable}}
It can be seen that the \bAR model shows again consistently strong performance on these
earlier days and is typically ranked first for both 7 and 14 day forecasts.

\begin{figure*}[t]
\centering
\includegraphics[width=.9\textwidth]{img/counties_bar_mae.png}
\caption{\label{fig:mae-google}Comparions of \bAR model to forecasts from Google Cloud AI and Columbia for 7 and 14 day horizons and earlier forecast dates. Forecast quality is measured in MAE where the absolute errors are averaged over all counties.}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=.9\columnwidth]{img/us_mae_granger_ablation/us_mae_granger_ablation.png}
\hfil
\includegraphics[width=.9\columnwidth]{img/us_mae_loss_ablation/us_mae_granger_ablation.png}
\caption{\label{fig:mae-covidhub-granger}Comparison of \bAR model with (blue) and without (magenta) Granger regularization. Forecast quality is measured in MAE.}
\end{figure*}


\paragraph{Ablations} In addition to comparisons to state-of-the-art
county-level forecasts, we also evaluate the contributions of different aspects
of our model. First, we test the effect of the relational autoregressive part.
For this purpose, we trained additional models were we disabled the relational
part (by setting \(\forall i \neq j: w_{ij} = 0\)) and compared their forecasts to
the full model of \Cref{eq:beta-ar}. To measure the relative improvement of the
full model over the non-relational model, we compute then the relative error of
both models, e.g.,
\begin{equation*}
    \text{Relative Mean Absolute Error} = \frac{\text{MAE}_{\text{full}}}{\text{MAE}_\text{non-relational}}
\end{equation*}
It can be seen from \Cref{fig:quality-ratio} that full model offers substantial
improvements over the non-relational model as the relative forecast quality
grows exponentially with the forecasting horizon. While the non-relational model
can offer acceptable forecast for horizons of 1-2 days, it quickly deteriorates
with larger horizons. This show the importance of the relational component for
disentangling the different growth factors and learning high quality models.

\begin{figure}[tb]
\centering
\includegraphics[width=.7\linewidth]{img/quality_ratio.png}
\caption{\label{fig:quality-ratio}Relative Error (MAE and RMSE) of the fully relational \bAR model compared to a non-relational variant.}
\end{figure}


In addition to the non-relational component, we also evaluated the contributions
of the logit-normal regularization method. For this purpose, we trained a model
where we explicitly set the reqularization parameter \(\sigma = 0\). We then
compare the forecast quality to the standard model where the regularization
parameter has been selected via cross-validation. \Cref{fig:mae-covidhub-granger}
shows the results of the comparison. It can be seen that the logit-normal
regularization can be very beneficial to improve forecast quality. While the
differences to the standard model are much smaller than for the non-relational
model, the addition of the regularization term can lead to substantial
improvements, especially for horizons of 13 days and longer.

Finally, we also evaluated the contributions of the Negative Binomial
distribution compared to a standard Poisson distribution for modeling confirmed
cases. Similar to the logit-normalization method, we trained an additional model
with Poisson likelihood and compared the forecast quality to the standard model.
It can be seen from \Cref{fig:mae-covidhub-loss}, that Negative Binomial likelihood
significantly improves the quality of the model over all forecast horizons. This
is likely due to the fact that the Negative Binomial can better model the noise
in the observed data, while the stricter Poisson likelihood causes the
(recurrent) model to overfit to these variations.

\begin{figure}[t]
\centering
\caption{\label{fig:mae-covidhub-loss}Comparison of \bAR model with Negative Binomial (blue) and Poisson (magenta) likelihood. Forecast quality is measured in MAE.}
\end{figure}


\begin{figure}
\includegraphics[width=\columnwidth]{img/betas.png}
\caption{Evolution of \(\beta\) over time}
\end{figure}


\input{reichlab_eval}
